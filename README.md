<p align="center">

# IEF: Impact Evaluation Framework: Attest, Measure, Evaluate, Compare

</p>

<div align="center">
    <h4>
        <a href="/CONTRIBUTING.md">
            üë• Framework
        </a>
        <span>&nbsp;&nbsp;|&nbsp;&nbsp;</span>
        <a href="/CODE_OF_CONDUCT.md">
            ü§ù Mechanism
        </a>
        <span>&nbsp;&nbsp;|&nbsp;&nbsp;</span>
        <a href="https://github.com/semaphore-protocol/semaphore/contribute">
            üîé Research
        </a>
        <span>&nbsp;&nbsp;|&nbsp;&nbsp;</span>
        <a href="https://semaphore.appliedzkp.org/discord">
            üó£Ô∏è Next Steps
        </a>
    </h4>
</div>

| IEF is a platform designed to enable RetroPGF Round 3 nominees to evaluate a random set of peer nominees projects impact anonymously. Through this evaluation, they can attest to the correct self-evaluation of others and help improve the current information gap badgeholders that are not familiar with a specific category might have.
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

Leveraging UniRep, IEF ensures that only projects who have submitted a self-reported impact evaluation, and are therefore familiar with the Impact Evaluation Framework, can evaluate peers **in an anonymous manner** using zero knowledge. 



## üì¶ Problem(s)

                                 Measuring the impact generated by public goods projects in web3 is hard. 
This obstacle was evident during Optimism's RetroPGF Round2, where measuring impact was a [challenge](https://optimism.mirror.xyz/7v1DehEY3dpRcYFhqWrVNc9Qj94H2L976LKlWH1FX-8) both to Nominees (who didn't know how to measure their impact for others to see) and for badgeholders (who didn't know how to assess the impact a project had). 

## Why is this a problem?
  An innability to measure the impact a public goods project creates ripple effects that harm the Ethereum Ecosystem ability to thrive in the future by:
  
  ### Underfunding Public Goods: 

Projects that are unable to measure and communicate the impact of the work they have done will leave money on the table that badgeholders would have been willing to commit given the right information. Taken to the extreme, if not enough resources are allocated to a project, the public good being provided by it may cease to exist. 

 ### Top-down incomplete evaluations: 
 
In the second round of RetroPGF, badgeholders were given limited guidance on how to assess the impact of the proposed projects. This presented a dual-edged problem. 

On one hand, the badgeholders, responsible for evaluating the projects, encountered difficulties when they had to assess projects in fields that were outside of their areas of expertise or familiarity. Without sufficient resources or context-specific knowledge, their ability to thoroughly evaluate the effectiveness and potential impact of these projects was impaired. 

On the other hand, and perhaps more critically, the nominees who proposed these projects faced top-down evaluations that might not have taken into account Key Performance Indicators (KPIs) significant to their on-the-ground operations. This issue stemmed from a potential disconnect between the evaluators' perspectives and the realities of hands-on fieldwork. 

There are specific indicators and factors that are vitally important and unique to ground-level work, which might not be visible, known, or may even be dismissed as irrelevant from the standpoint of someone who hasn't been involved in such in-situ operations. Consequently, these evaluations might have overlooked some crucial aspects of the projects, thereby affecting the comprehensiveness and accuracy of the assessments.

### Inability to scale:

During the RetroPGF process, the limited amount of information supplied to badgeholders posed a significant challenge. It necessitated an increased commitment of time as badgeholders needed to liaise with their counterparts to determine an effective approach for assessing the impact of projects. They were compelled to sift through the provided information, identify any gaps in data, and deliberate over which metrics should be the determining factors in the allocation of votes.

This process was time-consuming and could potentially become unmanageable as the number of projects involved in RetroPGF grows. The design and expectation of RetroPGF is to see an increase in participating projects. However, with the current system, it may result in an overwhelming workload for badgeholders. They may find it increasingly challenging to thoroughly review each project and thoughtfully distribute their votes due to time constraints.

As it stands, without more comprehensive guidelines and support, the expanding scope of RetroPGF might exceed the badgeholders' capacity to perform careful, conscientious evaluations, compromising the effectiveness and fairness of the entire process. There is an urgent need to streamline and enhance the evaluation framework to ensure its scalability and efficacy as the initiative grows.

## üí° Solution

IEF proposes a 3-step process to address the challenge in measuring the impact of public goods projects applying for RetroPGF. 
This process is only possible with the creation, and upkeep of an Impact Evaluation Framework that provides opt-in guidelines on a) how KPI's to measure impact are created and measured (at a technical level), b) offers a non-exhaustive set of KPI's that are commonly used by projects to evaluate their impact, and the logic behind those KPI's (these can be used as inspiration on how to create new KPI's), c) the importance of empowering operators to self-assess, d) guidelines to generate both quantitative and qualitative evaluations, e) the dangers of over-meassuring. 

Leveraging this Impact Evaluation Framework, I suggest the following process to evaluate impact: 


<table>
    <th>Stage</th>
    <th>Name</th>
    <th>Description</th>
    <tbody>
        <tr>
            <td>
                1
            </td>
            <td>
               Self-reported impact evaluation
            </td>
            <td>    
               Nominees will create a self-reported impact evaluation based on the impact their projects have had on the Ethereum Ecosystem and their ripple or direct impact on the Optimism Ecosystem. 
            </td>
        </tr>
        <tr>
            <td>
               2
                </a>
            </td>
            <td>
                Random, anonymous, peer evaluation
            </td>
            <td>    
                Each project that has submitted a self-reported evaluation and has registered on the IEF platform will, at random, be assigned to evaluate 5 peer projects self-reported evaluations. Peer evaluations will be anonymous, meaning the evaluated projects will be unable to tell who evaluated them, which helps reduce collusion and retaliation. They will also be preserved as binary attestations within the UniRep protocol: meaning an evaluating project may only approve or dissaprove the self report issued by a peer. 
            </td>
        </tr>
        <tr>
            <td>
                3
            </td>
            <td>
Badgeholders evaluation of a random set of self-reported evaluations along the peer attestations to those reports.              </td>
            <td>    
              Optimism Badgeholders will receive a capped random set of projects to evaluate. By capping the number of projects to be evaluated, badgeholders do not need to spread themselves thin. Additionally, through the use of the Impact Evaluation Framework and the results of the peer evaluation, badgeholders will have an increased amount of information to base their decisions on. 
            </td>
           <tbody>
</table>

## üõ† Process



## üìú Next Steps

1. Enable the use of POAPs as a data point to increase a projects reputation on the UniRep protocol. Having certain POAPs should contribute to giving more credibility to a projects evaluation of their peers. 
2. Create user lists within the UniRep code so that projects are 1) able to evaluate peers in multiple categories AND have different reputation scores for each category, based on their qualifications in each of these categories. 
3. Enable the carrying over of the reputation score from one epoch to the next. 
4. Enable an increase in reputation per category by having participated in multiple peer evaluation rounds. 

### Important


Having anonymous peer review may prevent collusion and retaliation, but can also prevent the development of a healthy and nurturing community feedback loop that enhances projects work and improvement on previous iterations. 

Therefore, I suggest running an A|B test in which a segment of evaluators are anonymous and another isn't to explore which group yields better results. 
